{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba0dcd2-cc93-44a1-bfa6-05cd9ff1f65b",
   "metadata": {},
   "source": [
    "# Лабаораторная работа №2: Классификация аудио"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ba5b3-d1be-42c6-b9c4-75b4a4fc3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install datasets torchaudio torch torchvision scikit-learn transformers accelerate --upgrade\n",
    "\n",
    "import numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio.transforms as T\n",
    "from datasets import load_dataset, Audio\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "SR = 16000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c914fbb-38a0-4362-8553-d7d36b936c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"danavery/urbansound8K\")\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=SR))\n",
    "\n",
    "def by_fold(d, folds): return d.filter(lambda ex: ex[\"fold\"] in folds)\n",
    "train_raw = by_fold(ds[\"train\"], list(range(1,9)))   # 1..8\n",
    "valid_raw = by_fold(ds[\"train\"], [9])                # 9\n",
    "test_raw  = by_fold(ds[\"train\"], [10])               # 10\n",
    "\n",
    "labels = sorted(list(set(train_raw[\"class\"])))\n",
    "label2id = {l:i for i,l in enumerate(labels)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "label_col = \"class\"\n",
    "\n",
    "len(train_raw), len(valid_raw), len(test_raw), len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce166c4b-c98e-417c-b2e0-8e354c66db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MELS, N_FFT, HOP = 64, 1024, 256\n",
    "FMIN, FMAX = 20.0, SR/2\n",
    "mel = T.MelSpectrogram(sample_rate=SR, n_fft=N_FFT, hop_length=HOP,\n",
    "                       n_mels=N_MELS, f_min=FMIN, f_max=FMAX, power=2.0)\n",
    "to_db = T.AmplitudeToDB(top_db=80)\n",
    "\n",
    "def logmel_stats(ex):\n",
    "    wav = torch.tensor(ex[\"audio\"][\"array\"], dtype=torch.float32).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        S = to_db(mel(wav)).squeeze(0).numpy().astype(\"float32\")  # (M,T)\n",
    "    m, s = S.mean(), S.std() + 1e-6\n",
    "    S = (S - m) / s\n",
    "    feat = np.concatenate([S.mean(1), S.std(1)]).astype(\"float32\")\n",
    "    return {\"feat\": feat, \"y\": label2id[ex[label_col]]}\n",
    "\n",
    "def to_xy(split):\n",
    "    a = split.map(logmel_stats, remove_columns=split.column_names)\n",
    "    return np.stack(a[\"feat\"]), np.array(a[\"y\"])\n",
    "\n",
    "X_tr, y_tr = to_xy(train_raw)\n",
    "X_va, y_va = to_xy(valid_raw)\n",
    "X_te, y_te = to_xy(test_raw)\n",
    "\n",
    "clf = Pipeline([(\"scaler\", StandardScaler()), (\"svm\", LinearSVC())])\n",
    "clf.fit(X_tr, y_tr)\n",
    "print(\"LinearSVC  VAL acc:\", accuracy_score(y_va, clf.predict(X_va)))\n",
    "print(\"LinearSVC  TEST acc:\", accuracy_score(y_te, clf.predict(X_te)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a98a92-94fc-4d9d-a797-8727f44d45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawWaveDS(Dataset):\n",
    "    def __init__(self, split): self.s = split\n",
    "    def __len__(self): return len(self.s)\n",
    "    def __getitem__(self, i):\n",
    "        ex = self.s[i]\n",
    "        return torch.tensor(ex[\"audio\"][\"array\"], dtype=torch.float32), label2id[ex[label_col]]\n",
    "\n",
    "def pad_collate(batch):\n",
    "    waves, ys = zip(*batch)\n",
    "    L = max(w.shape[0] for w in waves)\n",
    "    X = torch.zeros(len(waves), L)\n",
    "    for i,w in enumerate(waves): X[i,:w.shape[0]] = w\n",
    "    return X, torch.tensor(ys, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(RawWaveDS(train_raw), batch_size=32, shuffle=True,  num_workers=0, collate_fn=pad_collate)\n",
    "val_loader   = DataLoader(RawWaveDS(valid_raw), batch_size=32, shuffle=False, num_workers=0, collate_fn=pad_collate)\n",
    "test_loader  = DataLoader(RawWaveDS(test_raw),  batch_size=32, shuffle=False, num_workers=0, collate_fn=pad_collate)\n",
    "\n",
    "mel_gpu = T.MelSpectrogram(sample_rate=SR, n_fft=N_FFT, hop_length=HOP,\n",
    "                           n_mels=N_MELS, f_min=FMIN, f_max=FMAX, power=2.0).to(device)\n",
    "to_db_gpu = T.AmplitudeToDB(top_db=80).to(device)\n",
    "FIX_T = 128\n",
    "def to_logmel_batch(xb_1d):\n",
    "    S = to_db_gpu(mel_gpu(xb_1d))          # (B,M,Tm)\n",
    "    m = S.mean(dim=(1,2), keepdim=True); s = S.std(dim=(1,2), keepdim=True).clamp_min(1e-6)\n",
    "    S = (S - m)/s\n",
    "    Tm = S.size(-1)\n",
    "    if Tm < FIX_T: S = F.pad(S, (0, FIX_T-Tm))\n",
    "    else: S = S[:, :, :FIX_T]\n",
    "    return S.unsqueeze(1)                  # (B,1,M,FIX_T)\n",
    "\n",
    "class VGGishAudio(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.h = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.h(self.f(x))\n",
    "\n",
    "\n",
    "model = VGGishAudio(len(label2id)).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "def run_epoch(loader, train=True, tag=\"\"):\n",
    "    model.train() if train else model.eval()\n",
    "    loss_sum, correct, total = 0.0, 0, 0\n",
    "    for xb_wave, yb in tqdm(loader, desc=tag, leave=False):\n",
    "        xb_wave, yb = xb_wave.to(device), yb.to(device)\n",
    "        xb = to_logmel_batch(xb_wave)\n",
    "        if train: opt.zero_grad(set_to_none=True)\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(xb); loss = crit(logits, yb)\n",
    "            if train: loss.backward(); opt.step()\n",
    "        loss_sum += loss.item()*xb.size(0)\n",
    "        pred = logits.argmax(1); correct += (pred==yb).sum().item(); total += xb.size(0)\n",
    "    return loss_sum/total, correct/total\n",
    "epochs = 2\n",
    "for ep in range(1, epochs):\n",
    "    tr_l, tr_a = run_epoch(train_loader, True,  f\"train {ep}/{epochs}\")\n",
    "    va_l, va_a = run_epoch(val_loader,   False, f\"valid {ep}/{epochs}\")\n",
    "    print(f\"Ep {ep:02d}: train {tr_l:.4f}/{tr_a:.3f} | valid {va_l:.4f}/{va_a:.3f}\")\n",
    "\n",
    "# Тест\n",
    "model.eval(); y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb_wave, yb in tqdm(test_loader, desc=\"test\", leave=False):\n",
    "        logits = model(to_logmel_batch(xb_wave.to(device)))\n",
    "        y_true += yb.numpy().tolist()\n",
    "        y_pred += logits.argmax(1).cpu().numpy().tolist()\n",
    "print(\"CNN TEST acc:\", accuracy_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d71f3-0b08-4a6f-9f94-86d55a83d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "ast = AutoModelForAudioClassification.from_pretrained(\n",
    "    MODEL_ID, num_labels=len(labels), label2id=label2id, id2label=id2label,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "def map_raw(ex):\n",
    "    return {\n",
    "        \"wave\": np.asarray(ex[\"audio\"][\"array\"], dtype=\"float32\"),\n",
    "        \"labels\": label2id[ex[label_col]],\n",
    "    }\n",
    "\n",
    "tr_ast = train_raw.map(map_raw, remove_columns=train_raw.column_names, load_from_cache_file=False)\n",
    "va_ast = valid_raw.map(map_raw, remove_columns=valid_raw.column_names, load_from_cache_file=False)\n",
    "te_ast = test_raw .map(map_raw, remove_columns=test_raw .column_names, load_from_cache_file=False)\n",
    "\n",
    "\n",
    "\n",
    "def collate_proc(batch):\n",
    "    def extract_wave(b):\n",
    "        if \"wave\" in b:\n",
    "            return b[\"wave\"]\n",
    "        if \"audio\" in b and isinstance(b[\"audio\"], dict) and \"array\" in b[\"audio\"]:\n",
    "            return b[\"audio\"][\"array\"]\n",
    "        if \"input_values\" in b: \n",
    "            return b[\"input_values\"]\n",
    "        raise KeyError(f\"Expected one of keys ['wave','audio','input_values'], got {list(b.keys())}\")\n",
    "\n",
    "    waves  = [extract_wave(b) for b in batch]\n",
    "    labels = [b[\"labels\"] if \"labels\" in b else b[\"label\"] for b in batch]\n",
    "\n",
    "    inputs = processor(waves, sampling_rate=SR, return_tensors=\"pt\",\n",
    "                       padding=True, truncation=True)\n",
    "    inputs[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, -1)\n",
    "    return {\"accuracy\": float(accuracy_score(labels, preds)),\n",
    "            \"f1_macro\": float(f1_score(labels, preds, average=\"macro\"))}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"ast_us8k_ft\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_steps=500,\n",
    "    logging_steps=200,\n",
    "    fp16=(device.type==\"cuda\"),\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    remove_unused_columns=False,   \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=ast,\n",
    "    args=args,\n",
    "    train_dataset=tr_ast,\n",
    "    eval_dataset=va_ast,\n",
    "    data_collator=collate_proc,    \n",
    "    processing_class=processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"AST VALID:\", trainer.evaluate(va_ast))\n",
    "print(\"AST TEST :\", trainer.evaluate(te_ast))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be72f1-4cc9-4f44-a0e7-64d0513c2d30",
   "metadata": {},
   "source": [
    "## Задание 1. Таблица сравнения и графики обучения (CNN vs AST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb7e710-fca0-4c3a-96da-139f2c44f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(1.1): Таблица сравнения (VAL/TEST) для CNN и AST\n",
    "# Подставьте ваши значения метрик. Если их нет — посчитайте/извлеките выше.\n",
    "# Требуемые поля: cnn_val_acc, cnn_val_f1, cnn_test_acc, cnn_test_f1\n",
    "#                 ast_val_acc, ast_val_f1, ast_test_acc, ast_test_f1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Пример: замените None на ваши переменные/числа ---\n",
    "cnn_val_acc  = None  # TODO\n",
    "cnn_val_f1   = None  # TODO\n",
    "cnn_test_acc = None  # TODO\n",
    "cnn_test_f1  = None  # TODO\n",
    "\n",
    "ast_val_acc  = None  # TODO\n",
    "ast_val_f1   = None  # TODO\n",
    "ast_test_acc = None  # TODO\n",
    "ast_test_f1  = None  # TODO\n",
    "\n",
    "results = pd.DataFrame([\n",
    "    [\"CNN (VGG)\", cnn_val_acc, cnn_val_f1, cnn_test_acc, cnn_test_f1],\n",
    "    [\"AST FT\",    ast_val_acc, ast_val_f1, ast_test_acc, ast_test_f1],\n",
    "], columns=[\"Model\", \"VAL acc\", \"VAL f1_macro\", \"TEST acc\", \"TEST f1_macro\"])\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb85842-d1d3-4356-8d56-d675d50da56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(1.2): Графики обучения CNN (loss/acc по эпохам)\n",
    "#  Логгируйте значения из вашего цикла обучения.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Пример построения (раскомментируйте и подставьте):\n",
    "# plt.figure(); plt.plot(history[\"tr_loss\"]); plt.plot(history[\"va_loss\"]); \n",
    "# plt.title(\"CNN — Loss\"); plt.legend([\"train\",\"valid\"]); plt.xlabel(\"epoch\"); plt.show()\n",
    "# plt.figure(); plt.plot(history[\"tr_acc\"]); plt.plot(history[\"va_acc\"]);\n",
    "# plt.title(\"CNN — Accuracy\"); plt.legend([\"train\",\"valid\"]); plt.xlabel(\"epoch\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f29c69-aecf-44f8-a163-1ebf04147840",
   "metadata": {},
   "source": [
    "## Задание 2. Эксперимент (выберите один вариант)\n",
    "\n",
    "**Вариант A:** добавить SpecAugment в обучение CNN.  \n",
    "**Вариант B:** изменить параметры мел-спектрограмм (например, `N_MELS`, `HOP`, `N_FFT`) и переобучить CNN.\n",
    "\n",
    "Оформите гипотезу → что меняете → метрики ДО/ПОСЛЕ → краткий вывод.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d4f974-2f48-4249-9e0a-907edfa534f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(2.A): SpecAugment для CNN\n",
    "# Вставьте вызов в обучающий цикл CNN только для train-batch (до forward).\n",
    "# Подпишите конфигурацию масок.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def specaugment(x, time_mask=20, freq_mask=8, n_time_masks=1, n_freq_masks=1):\n",
    "    \"\"\"\n",
    "    x: (B, 1, n_mels, T) — мел-спектры\n",
    "    Возвращает аугментированный тензор.\n",
    "    \"\"\"\n",
    "    # TODO: реализовать маскирование по времени и по частоте (n_time_masks / n_freq_masks)\n",
    "    # Подсказка: зануляйте x[:, :, f0:f0+f, :] и x[:, :, :, t0:t0+t]\n",
    "    return x\n",
    "\n",
    "# Пример подключения в цикле:\n",
    "# xb = to_logmel_batch(xb_wave)\n",
    "# if train:\n",
    "#     xb = specaugment(xb, time_mask=..., freq_mask=...)\n",
    "# logits = model(xb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7697d-587a-4522-858c-ab1379557618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(2.B): Изменение мел-параметров для CNN\n",
    "# Укажите НОВЫЕ значения, пересоздайте мел-преобразования и переобучите CNN.\n",
    "\n",
    "# Пример: (замените на свои)\n",
    "# N_MELS_NEW = 80   # было 64\n",
    "# HOP_NEW    = 160  # было 256\n",
    "# N_FFT_NEW  = 1024 # по необходимости\n",
    "\n",
    "# 1) Пересоздайте MelSpectrogram/AmplitudeToDB с новыми параметрами\n",
    "# 2) Обновите функцию to_logmel_batch (если параметры зашиты)\n",
    "# 3) Переобучите CNN и посчитаете метрики (VAL/TEST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f97a97-89c7-4385-921d-0a031c78e0ff",
   "metadata": {},
   "source": [
    "### Отчёт по эксперименту\n",
    "**Вариант:** A (SpecAugment) / B (мел-параметры)  \n",
    "**Гипотеза:** …  \n",
    "**Конфигурация:** …  \n",
    "**Результаты (VAL/TEST, acc и macro-F1):** ДО → … | ПОСЛЕ → …  \n",
    "**Вывод (1–3 предложения):** …\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc8320-d534-4853-9a7f-299a3a971b02",
   "metadata": {},
   "source": [
    "## Задание 3. Инференс AST на собственных `.wav`\n",
    "Загрузите файл(ы), при необходимости ресемплируйте до 16kHz, сделайте топ-K предсказаний AST и прокомментируйте результаты.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4274fb9e-7682-43a9-a3e5-1f3ddf896932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(3): Инференс AST на своём .wav\n",
    "# Требуются: processor, ast (обученная модель), SR, id2label\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "def ast_predict_wav(path, top_k=3):\n",
    "    # 1) загрузка .wav\n",
    "    wav, sr = sf.read(path)\n",
    "    wav = wav.astype(np.float32)\n",
    "\n",
    "    # 2) ресемплинг при несоответствии частоты\n",
    "    #TODO\n",
    "\n",
    "    # 3) препроцессинг\n",
    "    inputs = processor(wav, sampling_rate=SR, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(ast.device) for k, v in inputs.items()}\n",
    "\n",
    "    # 4) инференс\n",
    "    ast.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = ast(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze(0)\n",
    "\n",
    "    # 5) топ-K\n",
    "    vals, idx = torch.topk(probs, k=min(top_k, probs.numel()))\n",
    "    vals, idx = vals.cpu().numpy(), idx.cpu().numpy()\n",
    "    return [(id2label[int(i)], float(v)) for i, v in zip(idx, vals)]\n",
    "\n",
    "# Пример:\n",
    "# ast_predict_wav(\"my_audio.wav\", top_k=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Audio Course",
   "language": "python",
   "name": "audiocourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
